{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9awWKKTRGXD"
      },
      "source": [
        "# 1. Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6aQVcY9RCId"
      },
      "source": [
        "## 1.0 Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W402iKJxzvpL"
      },
      "outputs": [],
      "source": [
        "!pip install alibi-detect\n",
        "!pip install easyfsl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PSFH_A1k301"
      },
      "source": [
        "## 1.1 Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwTjqaVOkljs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Set the base path to specify where we are working\n",
        "PROJECT_BASE_PATH = \"/content/drive/MyDrive/CS 229 Project\"  # Change to match your mounted drive layout\n",
        "\n",
        "# Set paths for our train and test data with proper relation to our mounted drive\n",
        "TRAIN_PATH = os.path.join(PROJECT_BASE_PATH, \"binned_complexities_easyset_train.json\")\n",
        "TEST_PATH = os.path.join(PROJECT_BASE_PATH, \"binned_complexities_easyset_test.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBbmi8LKQ5NK"
      },
      "source": [
        "### a. (Run Once) Split Data\n",
        "You should not (and likely will get an error either way if you try to) repeat this step.\n",
        "\n",
        "Make sure you have all your files in hand so you do not have to reset your data and repeat this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ra3JCIxQ5g4"
      },
      "outputs": [],
      "source": [
        "# You'll need to copy over your PROJECT_BASE_PATH to here as well\n",
        "safeguard = True\n",
        "\n",
        "if not safeguard:\n",
        "  !python /content/drive/MyDrive/\"CS 229 Project\"/create_easyset_data.py\n",
        "  !python /content/drive/MyDrive/\"CS 229 Project\"/create_train_test_easyset.py\n",
        "else:\n",
        "  print(\"Safeguard is active. Skipping data split.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjlCqmygQso3"
      },
      "source": [
        "### b. (Optional) Verify Data Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y-5cwL6PpVI"
      },
      "outputs": [],
      "source": [
        "print(f\"{TRAIN_PATH = }\")\n",
        "print(f\"{TEST_PATH = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1r37sfWQxDE"
      },
      "source": [
        "## 1.2 Generate EasySet Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-NxsaQdPrbN"
      },
      "outputs": [],
      "source": [
        "from easyfsl.datasets import EasySet\n",
        "from torchvision import transforms\n",
        "\n",
        "image_size = 384\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor()\n",
        "        ,transforms.Normalize(**{\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]})\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_set = EasySet(TRAIN_PATH, image_size=image_size, transform=train_transform, training=True)\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor()\n",
        "        # ,transforms.Normalize(**{\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]})\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_set = EasySet(TEST_PATH, image_size=image_size, transform=test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvpchSy7QzQP"
      },
      "source": [
        "### (Optional) Validate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkfA5Uu5Pzn3"
      },
      "outputs": [],
      "source": [
        "# Train set checks\n",
        "print(f\"{train_set.number_of_classes() = }\")\n",
        "print(f\"{len(train_set) = }\\n\")\n",
        "\n",
        "# Test set checks\n",
        "print(f\"{test_set.number_of_classes() = }\")\n",
        "print(f\"{len(test_set) = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCj_xfbyQcw8"
      },
      "source": [
        "## 1.3 Get Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vKShgw8Qa7e"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Get our support images and labels\n",
        "support_images_list, support_labels_list = zip(*[train_set[i] for i in tqdm(range(len(train_set)), desc=\"Unpacking training (support) images\")])\n",
        "\n",
        "# Restructure the images and labels to be tensors\n",
        "support_images = torch.stack(support_images_list, dim=0, out=None)\n",
        "support_labels = torch.Tensor(support_labels_list)\n",
        "\n",
        "# Get our query images and labels\n",
        "query_images_list, query_labels_list = zip(*[test_set[i] for i in tqdm(range(len(test_set)), desc=\"Unpacking testing (query) images\")])\n",
        "\n",
        "# Restructure the images and labels to be tensors\n",
        "query_images = torch.stack(query_images_list, dim=0, out=None)\n",
        "query_labels = torch.Tensor(query_labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0EpcUKovUlx"
      },
      "outputs": [],
      "source": [
        "# Temp code, rename above variables please\n",
        "X_train = support_images\n",
        "X_test = query_images\n",
        "\n",
        "n, c, w, h = X_train.shape\n",
        "\n",
        "X_train = X_train.reshape((n, w, h, c))\n",
        "\n",
        "n_t, c_t, w_t, h_t = X_test.shape\n",
        "\n",
        "X_test = X_test.reshape((n_t, w_t, h_t, c_t))\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPHbJ3N5zcKn"
      },
      "source": [
        "## 1.4 Get Encoder/Decoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL32T3xkv-Sx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, Dense, Reshape, Conv2DTranspose, Flatten\n",
        "\n",
        "encoder_net = tf.keras.Sequential(\n",
        "  [\n",
        "      InputLayer(input_shape=(image_size, image_size, 3)),\n",
        "      Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
        "      Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
        "      Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu)\n",
        "  ])\n",
        "\n",
        "encoded_size = image_size // (2**3)\n",
        "\n",
        "decoder_net = tf.keras.Sequential(\n",
        "  [\n",
        "      InputLayer(input_shape=(encoded_size, encoded_size, 512)),\n",
        "      Reshape(target_shape=(encoded_size, encoded_size, 512)),\n",
        "      Conv2DTranspose(256, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
        "      Conv2DTranspose(128, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
        "      Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
        "      Conv2DTranspose(3, 4, strides=1, padding='same', activation='sigmoid')  # No stride here to maintain the size\n",
        "  ])\n",
        "\n",
        "class MultistackAutoencoder(tf.keras.Model):\n",
        "  def __init__(self, num_stacks=3):\n",
        "    super().__init__()\n",
        "    self.layers = []\n",
        "    for _ in range(num_stacks):\n",
        "      # Encoder\n",
        "      self.layers.append(InputLayer(input_shape=(image_size, image_size, 3)))\n",
        "      self.layers.append(Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu))\n",
        "      self.layers.append(Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu))\n",
        "      self.layers.append(Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu))\n",
        "\n",
        "      # Decoder\n",
        "      self.layers.append(InputLayer(input_shape=(encoded_size, encoded_size, 512)))\n",
        "      self.layers.append(Reshape(target_shape=(encoded_size, encoded_size, 512)))\n",
        "      self.layers.append(Conv2DTranspose(256, 4, strides=2, padding='same', activation=tf.nn.relu))\n",
        "      self.layers.append(Conv2DTranspose(128, 4, strides=2, padding='same', activation=tf.nn.relu))\n",
        "      self.layers.append(Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu))\n",
        "      self.layers.append(Conv2DTranspose(3, 4, strides=1, padding='same', activation='sigmoid'))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    next_input = inputs\n",
        "    for layer_idx in range(len(self.layers)):\n",
        "      next_input = self.layers[layer_idx](next_input)\n",
        "\n",
        "    return next_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzGPrNxnkp6e"
      },
      "source": [
        "# 2. Model Construction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3c_AwW4mt6i"
      },
      "outputs": [],
      "source": [
        "load_saved_model = True\n",
        "saved_detector_type = \"outlier\"\n",
        "saved_detector_name = \"outlier_save\"\n",
        "saved_detector_path = os.path.join(PROJECT_BASE_PATH, saved_detector_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-daYY2Mc0OrO"
      },
      "outputs": [],
      "source": [
        "from alibi_detect.od import OutlierAE\n",
        "from alibi_detect.saving import save_detector, load_detector\n",
        "\n",
        "if load_saved_model:\n",
        "    print(\"Loading detector\")\n",
        "    outlier_detector = load_detector(saved_detector_path)\n",
        "else:\n",
        "    outlier_detector = OutlierAE(threshold=0.1,\n",
        "                                encoder_net=encoder_net,\n",
        "                                decoder_net=decoder_net,\n",
        "                                data_type=\"image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OluOEjLkq5U"
      },
      "source": [
        "### a. Infer Outlier Threshold\n",
        "Infers the threshold to classify outliers based on the training data. I decided not to since the transforms on the training data severely impacts the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEWqeBJK9lxh"
      },
      "outputs": [],
      "source": [
        "# Instead infer the threshold\n",
        "# outlier_detector.infer_threshold(tf.convert_to_tensor(X_train.numpy()), threshold_perc=90)\n",
        "# print(f\"Inferred Threshold: {outlier_detector.threshold}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYt0iz4olPxn"
      },
      "source": [
        "## 2.1 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlmf6WQU19OI"
      },
      "outputs": [],
      "source": [
        "if not load_saved_model:\n",
        "    outlier_detector.fit(X_train, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY57EaGRe37F"
      },
      "outputs": [],
      "source": [
        "if not load_saved_model:\n",
        "    save_detector(outlier_detector, os.path.join(PROJECT_BASE_PATH, \"outlier_save\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMJVSrHClV7H"
      },
      "source": [
        "# 3. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqz5F-_-llGU"
      },
      "source": [
        "## 3.1 Predictions\n",
        "Predict on our test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "row_ids = random.sample(range(0, X_test.shape[0]), 100)\n",
        "X_small_test = X_test.numpy()[row_ids, :]"
      ],
      "metadata": {
        "id": "vbevvqdEk380"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNfDX7H6LVAC"
      },
      "outputs": [],
      "source": [
        "results = outlier_detector.predict(tf.convert_to_tensor(X_small_test),\n",
        "                                   outlier_type=\"instance\",\n",
        "                                   return_feature_score=True,\n",
        "                                   return_instance_score=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy0U0rq6luba"
      },
      "source": [
        "## 3.2 Plot Outlier Instance Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tLQKShUMDke"
      },
      "outputs": [],
      "source": [
        "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image\n",
        "import numpy as np\n",
        "\n",
        "manual_threshold = 0.05 # remove for outlier_detector.threshold\n",
        "selected_threshold = manual_threshold\n",
        "# selected_threshold = outlier_detector.threshold\n",
        "\n",
        "plot_instance_score(results, np.zeros(X_small_test.shape[0],).astype(int), [\"normal\", \"outlier\"], selected_threshold)\n",
        "outlier_indices = np.where(results['data'][\"instance_score\"] > selected_threshold)[0]\n",
        "inlier_indices = np.where(results['data'][\"instance_score\"] <= selected_threshold)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kk1KwBxM7-G"
      },
      "outputs": [],
      "source": [
        "X_recon = outlier_detector.ae(tf.convert_to_tensor(X_small_test)).numpy()\n",
        "outlier_samples = np.random.choice(outlier_indices, size=4)\n",
        "\n",
        "# Plot Outliers\n",
        "plot_feature_outlier_image(results,\n",
        "                           X_small_test,\n",
        "                           X_recon=X_recon,\n",
        "                           instance_ids=outlier_samples,  # pass a list with indices of instances to display\n",
        "                           n_channels=3,\n",
        "                           outliers_only=False)  # only show outlier predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LffEyx6Gpe_P"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "sample_outlier_photos = [test_set.images[idx] for idx in np.array(row_ids)[outlier_samples]]\n",
        "\n",
        "imgs = [mpimg.imread(sample_outlier_photos[i]) for i in range(4)]\n",
        "fig, axs = plt.subplots(4, 1)\n",
        "\n",
        "for i in range(4):\n",
        "  axs[i].imshow(imgs[i])\n",
        "  axs[i].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8GqRf6EgpVC"
      },
      "outputs": [],
      "source": [
        "# Plot inliers\n",
        "inlier_samples = np.random.choice(inlier_indices, size=4)\n",
        "\n",
        "plot_feature_outlier_image(results,\n",
        "                           X_small_test,\n",
        "                           X_recon=X_recon,\n",
        "                           instance_ids=inlier_samples,  # pass a list with indices of instances to display\n",
        "                           n_channels=3,\n",
        "                           outliers_only=False)  # only show outlier predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DekqAoSEfEG-"
      },
      "outputs": [],
      "source": [
        "sample_inlier_photos = [test_set.images[idx] for idx in np.array(row_ids)[inlier_samples]]\n",
        "\n",
        "imgs = [mpimg.imread(sample_inlier_photos[i]) for i in range(4)]\n",
        "fig, axs = plt.subplots(4, 1)\n",
        "\n",
        "for i in range(4):\n",
        "  axs[i].imshow(imgs[i])\n",
        "  axs[i].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdY5HZqh0eIY"
      },
      "outputs": [],
      "source": [
        "outlier_photos = [train_set.images[idx] for idx in outlier_indices]\n",
        "inlier_photos = [train_set.images[idx] for idx in inlier_indices]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PHOTO_STUFF = False"
      ],
      "metadata": {
        "id": "eZ2BVLVD1m4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERYdBvyeVuIR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "if SAVE_PHOTO_STUFF:\n",
        "  outlier_save_name = f\"ae_{manual_threshold}_outliers\"\n",
        "  inlier_save_name = f\"ae_{manual_threshold}_inliers\"\n",
        "\n",
        "  outlier_save_path = os.path.join(PROJECT_BASE_PATH, outlier_save_name + \".json\")\n",
        "  inlier_save_path = os.path.join(PROJECT_BASE_PATH, inlier_save_name + \".json\")\n",
        "\n",
        "  with open(outlier_save_path, 'w') as outlier_out_fp:\n",
        "      json.dump(outlier_photos, outlier_out_fp, indent=4)\n",
        "\n",
        "  with open(inlier_save_path, 'w') as inlier_out_fp:\n",
        "      json.dump(inlier_photos, inlier_out_fp, indent=4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}